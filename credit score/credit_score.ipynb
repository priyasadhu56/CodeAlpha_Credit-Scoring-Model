{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "880fb28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Job</th>\n",
       "      <th>Housing</th>\n",
       "      <th>Saving accounts</th>\n",
       "      <th>Checking account</th>\n",
       "      <th>Credit amount</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Purpose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>own</td>\n",
       "      <td>NaN</td>\n",
       "      <td>little</td>\n",
       "      <td>1169</td>\n",
       "      <td>6</td>\n",
       "      <td>radio/TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>female</td>\n",
       "      <td>2</td>\n",
       "      <td>own</td>\n",
       "      <td>little</td>\n",
       "      <td>moderate</td>\n",
       "      <td>5951</td>\n",
       "      <td>48</td>\n",
       "      <td>radio/TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>own</td>\n",
       "      <td>little</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2096</td>\n",
       "      <td>12</td>\n",
       "      <td>education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>free</td>\n",
       "      <td>little</td>\n",
       "      <td>little</td>\n",
       "      <td>7882</td>\n",
       "      <td>42</td>\n",
       "      <td>furniture/equipment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>free</td>\n",
       "      <td>little</td>\n",
       "      <td>little</td>\n",
       "      <td>4870</td>\n",
       "      <td>24</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Age     Sex  Job Housing Saving accounts Checking account  \\\n",
       "0           0   67    male    2     own             NaN           little   \n",
       "1           1   22  female    2     own          little         moderate   \n",
       "2           2   49    male    1     own          little              NaN   \n",
       "3           3   45    male    2    free          little           little   \n",
       "4           4   53    male    2    free          little           little   \n",
       "\n",
       "   Credit amount  Duration              Purpose  \n",
       "0           1169         6             radio/TV  \n",
       "1           5951        48             radio/TV  \n",
       "2           2096        12            education  \n",
       "3           7882        42  furniture/equipment  \n",
       "4           4870        24                  car  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r\"D:\\credit score\\german_credit_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "40f5125c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Unnamed: 0        1000 non-null   int64 \n",
      " 1   Age               1000 non-null   int64 \n",
      " 2   Sex               1000 non-null   object\n",
      " 3   Job               1000 non-null   int64 \n",
      " 4   Housing           1000 non-null   object\n",
      " 5   Saving accounts   817 non-null    object\n",
      " 6   Checking account  606 non-null    object\n",
      " 7   Credit amount     1000 non-null   int64 \n",
      " 8   Duration          1000 non-null   int64 \n",
      " 9   Purpose           1000 non-null   object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 78.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Show basic info (column names, types, missing values)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "df380655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0          Age          Job  Credit amount     Duration\n",
      "count  1000.000000  1000.000000  1000.000000    1000.000000  1000.000000\n",
      "mean    499.500000    35.546000     1.904000    3271.258000    20.903000\n",
      "std     288.819436    11.375469     0.653614    2822.736876    12.058814\n",
      "min       0.000000    19.000000     0.000000     250.000000     4.000000\n",
      "25%     249.750000    27.000000     2.000000    1365.500000    12.000000\n",
      "50%     499.500000    33.000000     2.000000    2319.500000    18.000000\n",
      "75%     749.250000    42.000000     2.000000    3972.250000    24.000000\n",
      "max     999.000000    75.000000     3.000000   18424.000000    72.000000\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics for numeric columns\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c54da57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0            0\n",
      "Age                   0\n",
      "Sex                   0\n",
      "Job                   0\n",
      "Housing               0\n",
      "Saving accounts     183\n",
      "Checking account    394\n",
      "Credit amount         0\n",
      "Duration              0\n",
      "Purpose               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4d5e704f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit amount\n",
      "1478    3\n",
      "1262    3\n",
      "1258    3\n",
      "1275    3\n",
      "1393    3\n",
      "       ..\n",
      "1459    1\n",
      "882     1\n",
      "3758    1\n",
      "1136    1\n",
      "4576    1\n",
      "Name: count, Length: 921, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['Credit amount'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824ca81f",
   "metadata": {},
   "source": [
    "### Data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1ced156e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\priya\\AppData\\Local\\Temp\\ipykernel_14848\\726507224.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "C:\\Users\\priya\\AppData\\Local\\Temp\\ipykernel_14848\\726507224.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "C:\\Users\\priya\\AppData\\Local\\Temp\\ipykernel_14848\\726507224.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "C:\\Users\\priya\\AppData\\Local\\Temp\\ipykernel_14848\\726507224.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "C:\\Users\\priya\\AppData\\Local\\Temp\\ipykernel_14848\\726507224.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "C:\\Users\\priya\\AppData\\Local\\Temp\\ipykernel_14848\\726507224.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mode()[0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Missing Value Handle\n",
    "# Numeric columns → median\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "for col in numeric_cols:\n",
    "    df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "# Categorical columns → mode\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    df[col].fillna(df[col].mode()[0], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6352f7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0          0\n",
      "Age                 0\n",
      "Sex                 0\n",
      "Job                 0\n",
      "Housing             0\n",
      "Saving accounts     0\n",
      "Checking account    0\n",
      "Credit amount       0\n",
      "Duration            0\n",
      "Purpose             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ca5745",
   "metadata": {},
   "source": [
    "### Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "aba4468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, drop_first=True) # Use oned hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fe1530",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4cfaebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df.drop('Credit amount', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ef7c0089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('Credit amount', axis=1)  # Features\n",
    "y = df['Credit amount']               # Target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7c5375",
   "metadata": {},
   "source": [
    "### Model Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb90c3a1",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "66a63d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in y_train: 750\n",
      "Total samples in y_train: 800\n",
      "Sample labels: [2051, 6148, 2058, 4110, 2063, 2064, 4113, 2069, 2073, 2080]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique labels in y_train:\", len(set(y_train)))\n",
    "print(\"Total samples in y_train:\", len(y_train))\n",
    "print(\"Sample labels:\", list(set(y_train))[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "626b6d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:213: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  y_type = type_of_target(y, input_name=\"y\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('penalty',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">penalty&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;l2&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('dual',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">dual&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tol',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">tol&nbsp;</td>\n",
       "            <td class=\"value\">0.0001</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('C',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">C&nbsp;</td>\n",
       "            <td class=\"value\">1.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('fit_intercept',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">fit_intercept&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('intercept_scaling',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">intercept_scaling&nbsp;</td>\n",
       "            <td class=\"value\">1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('class_weight',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">class_weight&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">random_state&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('solver',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">solver&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;lbfgs&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_iter',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_iter&nbsp;</td>\n",
       "            <td class=\"value\">1000</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('multi_class',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">multi_class&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;deprecated&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('warm_start',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">warm_start&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_jobs&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('l1_ratio',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">l1_ratio&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression with more iterations\n",
    "lr_model = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "lr_model.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "26f062b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 4015930.080895964\n",
      "R² Score: 0.3399192534633566\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R² Score:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "20201c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in y_train: 750\n",
      "Total samples in y_train: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:213: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  y_type = type_of_target(y, input_name=\"y\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         276       0.00      0.00      0.00       1.0\n",
      "         368       0.00      0.00      0.00       1.0\n",
      "         385       0.00      0.00      0.00       1.0\n",
      "         433       0.00      0.00      0.00       1.0\n",
      "         458       0.00      0.00      0.00       1.0\n",
      "         484       0.00      0.00      0.00       1.0\n",
      "         518       0.00      0.00      0.00       1.0\n",
      "         585       0.00      0.00      0.00       1.0\n",
      "         590       0.00      0.00      0.00       0.0\n",
      "         601       0.00      0.00      0.00       1.0\n",
      "         609       0.00      0.00      0.00       1.0\n",
      "         626       0.00      0.00      0.00       1.0\n",
      "         629       0.00      0.00      0.00       0.0\n",
      "         660       0.00      0.00      0.00       1.0\n",
      "         662       0.00      0.00      0.00       1.0\n",
      "         672       0.00      0.00      0.00       0.0\n",
      "         684       0.00      0.00      0.00       1.0\n",
      "         685       0.00      0.00      0.00       0.0\n",
      "         700       0.00      0.00      0.00       0.0\n",
      "         701       0.00      0.00      0.00       1.0\n",
      "         708       0.00      0.00      0.00       0.0\n",
      "         709       0.00      0.00      0.00       1.0\n",
      "         717       0.00      0.00      0.00       0.0\n",
      "         727       0.00      0.00      0.00       1.0\n",
      "         731       0.00      0.00      0.00       1.0\n",
      "         745       0.00      0.00      0.00       1.0\n",
      "         763       0.00      0.00      0.00       0.0\n",
      "         766       0.00      0.00      0.00       1.0\n",
      "         783       0.00      0.00      0.00       1.0\n",
      "         790       0.00      0.00      0.00       0.0\n",
      "         802       0.00      0.00      0.00       1.0\n",
      "         836       0.00      0.00      0.00       0.0\n",
      "         846       0.00      0.00      0.00       1.0\n",
      "         882       0.00      0.00      0.00       1.0\n",
      "         884       0.00      0.00      0.00       1.0\n",
      "         886       0.00      0.00      0.00       0.0\n",
      "         888       0.00      0.00      0.00       1.0\n",
      "         902       0.00      0.00      0.00       0.0\n",
      "         907       0.00      0.00      0.00       1.0\n",
      "         918       0.00      0.00      0.00       1.0\n",
      "         926       0.00      0.00      0.00       1.0\n",
      "         929       0.00      0.00      0.00       1.0\n",
      "         931       0.00      0.00      0.00       1.0\n",
      "         932       0.00      0.00      0.00       1.0\n",
      "         936       0.00      0.00      0.00       0.0\n",
      "         959       0.00      0.00      0.00       1.0\n",
      "         976       0.00      0.00      0.00       0.0\n",
      "        1007       0.00      0.00      0.00       1.0\n",
      "        1024       0.00      0.00      0.00       1.0\n",
      "        1048       0.00      0.00      0.00       1.0\n",
      "        1049       0.00      0.00      0.00       0.0\n",
      "        1098       0.00      0.00      0.00       1.0\n",
      "        1101       0.00      0.00      0.00       1.0\n",
      "        1123       0.00      0.00      0.00       0.0\n",
      "        1126       0.00      0.00      0.00       0.0\n",
      "        1136       0.00      0.00      0.00       0.0\n",
      "        1138       0.00      0.00      0.00       1.0\n",
      "        1154       0.00      0.00      0.00       1.0\n",
      "        1164       0.00      0.00      0.00       1.0\n",
      "        1168       0.00      0.00      0.00       0.0\n",
      "        1169       0.00      0.00      0.00       0.0\n",
      "        1190       0.00      0.00      0.00       1.0\n",
      "        1199       0.00      0.00      0.00       1.0\n",
      "        1200       0.00      0.00      0.00       1.0\n",
      "        1204       0.00      0.00      0.00       1.0\n",
      "        1224       0.00      0.00      0.00       0.0\n",
      "        1236       0.00      0.00      0.00       0.0\n",
      "        1237       0.00      0.00      0.00       0.0\n",
      "        1238       0.00      0.00      0.00       0.0\n",
      "        1244       0.00      0.00      0.00       1.0\n",
      "        1246       0.00      0.00      0.00       1.0\n",
      "        1258       0.00      0.00      0.00       0.0\n",
      "        1262       0.00      0.00      0.00       1.0\n",
      "        1264       0.00      0.00      0.00       0.0\n",
      "        1275       0.00      0.00      0.00       1.0\n",
      "        1278       0.00      0.00      0.00       0.0\n",
      "        1283       0.00      0.00      0.00       1.0\n",
      "        1285       0.00      0.00      0.00       0.0\n",
      "        1287       0.00      0.00      0.00       0.0\n",
      "        1295       0.00      0.00      0.00       2.0\n",
      "        1297       0.00      0.00      0.00       1.0\n",
      "        1318       0.00      0.00      0.00       0.0\n",
      "        1323       0.00      0.00      0.00       1.0\n",
      "        1331       0.00      0.00      0.00       0.0\n",
      "        1337       0.00      0.00      0.00       0.0\n",
      "        1338       0.00      0.00      0.00       1.0\n",
      "        1358       0.00      0.00      0.00       0.0\n",
      "        1361       0.00      0.00      0.00       0.0\n",
      "        1364       0.00      0.00      0.00       0.0\n",
      "        1374       0.00      0.00      0.00       1.0\n",
      "        1376       0.00      0.00      0.00       1.0\n",
      "        1386       0.00      0.00      0.00       0.0\n",
      "        1391       0.00      0.00      0.00       1.0\n",
      "        1393       0.00      0.00      0.00       1.0\n",
      "        1403       0.00      0.00      0.00       0.0\n",
      "        1409       0.00      0.00      0.00       0.0\n",
      "        1410       0.00      0.00      0.00       0.0\n",
      "        1413       0.00      0.00      0.00       1.0\n",
      "        1418       0.00      0.00      0.00       0.0\n",
      "        1433       0.00      0.00      0.00       1.0\n",
      "        1449       0.00      0.00      0.00       1.0\n",
      "        1453       0.00      0.00      0.00       1.0\n",
      "        1455       0.00      0.00      0.00       0.0\n",
      "        1474       0.00      0.00      0.00       0.0\n",
      "        1478       0.00      0.00      0.00       1.0\n",
      "        1494       0.00      0.00      0.00       1.0\n",
      "        1498       0.00      0.00      0.00       0.0\n",
      "        1503       0.00      0.00      0.00       1.0\n",
      "        1520       0.00      0.00      0.00       0.0\n",
      "        1521       0.00      0.00      0.00       0.0\n",
      "        1525       0.00      0.00      0.00       1.0\n",
      "        1530       0.00      0.00      0.00       1.0\n",
      "        1532       0.00      0.00      0.00       1.0\n",
      "        1533       0.00      0.00      0.00       1.0\n",
      "        1534       0.00      0.00      0.00       0.0\n",
      "        1538       0.00      0.00      0.00       0.0\n",
      "        1542       0.00      0.00      0.00       1.0\n",
      "        1544       0.00      0.00      0.00       0.0\n",
      "        1546       0.00      0.00      0.00       1.0\n",
      "        1574       0.00      0.00      0.00       1.0\n",
      "        1597       0.00      0.00      0.00       1.0\n",
      "        1603       0.00      0.00      0.00       1.0\n",
      "        1655       0.00      0.00      0.00       0.0\n",
      "        1657       0.00      0.00      0.00       1.0\n",
      "        1736       0.00      0.00      0.00       1.0\n",
      "        1743       0.00      0.00      0.00       1.0\n",
      "        1766       0.00      0.00      0.00       1.0\n",
      "        1768       0.00      0.00      0.00       0.0\n",
      "        1804       0.00      0.00      0.00       1.0\n",
      "        1817       0.00      0.00      0.00       0.0\n",
      "        1823       0.00      0.00      0.00       1.0\n",
      "        1835       0.00      0.00      0.00       1.0\n",
      "        1845       0.00      0.00      0.00       1.0\n",
      "        1851       0.00      0.00      0.00       1.0\n",
      "        1860       0.00      0.00      0.00       0.0\n",
      "        1864       0.00      0.00      0.00       0.0\n",
      "        1867       0.00      0.00      0.00       0.0\n",
      "        1872       0.00      0.00      0.00       1.0\n",
      "        1880       0.00      0.00      0.00       1.0\n",
      "        1881       0.00      0.00      0.00       1.0\n",
      "        1908       0.00      0.00      0.00       1.0\n",
      "        1913       0.00      0.00      0.00       1.0\n",
      "        1919       0.00      0.00      0.00       0.0\n",
      "        1922       0.00      0.00      0.00       0.0\n",
      "        1925       0.00      0.00      0.00       0.0\n",
      "        1928       0.00      0.00      0.00       1.0\n",
      "        1935       0.00      0.00      0.00       0.0\n",
      "        1938       0.00      0.00      0.00       1.0\n",
      "        1957       0.00      0.00      0.00       0.0\n",
      "        2012       0.00      0.00      0.00       1.0\n",
      "        2030       0.00      0.00      0.00       1.0\n",
      "        2069       0.00      0.00      0.00       0.0\n",
      "        2116       0.00      0.00      0.00       1.0\n",
      "        2122       0.00      0.00      0.00       1.0\n",
      "        2124       0.00      0.00      0.00       1.0\n",
      "        2133       0.00      0.00      0.00       1.0\n",
      "        2141       0.00      0.00      0.00       0.0\n",
      "        2171       0.00      0.00      0.00       2.0\n",
      "        2181       0.00      0.00      0.00       0.0\n",
      "        2186       0.00      0.00      0.00       0.0\n",
      "        2214       0.00      0.00      0.00       0.0\n",
      "        2221       0.00      0.00      0.00       1.0\n",
      "        2225       0.00      0.00      0.00       1.0\n",
      "        2241       0.00      0.00      0.00       1.0\n",
      "        2246       0.00      0.00      0.00       1.0\n",
      "        2249       0.00      0.00      0.00       1.0\n",
      "        2302       0.00      0.00      0.00       1.0\n",
      "        2303       0.00      0.00      0.00       1.0\n",
      "        2323       0.00      0.00      0.00       1.0\n",
      "        2325       0.00      0.00      0.00       1.0\n",
      "        2327       0.00      0.00      0.00       0.0\n",
      "        2384       0.00      0.00      0.00       2.0\n",
      "        2389       0.00      0.00      0.00       1.0\n",
      "        2390       0.00      0.00      0.00       0.0\n",
      "        2406       0.00      0.00      0.00       0.0\n",
      "        2415       0.00      0.00      0.00       0.0\n",
      "        2427       0.00      0.00      0.00       1.0\n",
      "        2463       0.00      0.00      0.00       1.0\n",
      "        2511       0.00      0.00      0.00       0.0\n",
      "        2515       0.00      0.00      0.00       1.0\n",
      "        2538       0.00      0.00      0.00       1.0\n",
      "        2570       0.00      0.00      0.00       0.0\n",
      "        2576       0.00      0.00      0.00       0.0\n",
      "        2600       0.00      0.00      0.00       0.0\n",
      "        2622       0.00      0.00      0.00       0.0\n",
      "        2629       0.00      0.00      0.00       0.0\n",
      "        2684       0.00      0.00      0.00       1.0\n",
      "        2687       0.00      0.00      0.00       0.0\n",
      "        2697       0.00      0.00      0.00       1.0\n",
      "        2708       0.00      0.00      0.00       0.0\n",
      "        2712       0.00      0.00      0.00       1.0\n",
      "        2745       0.00      0.00      0.00       1.0\n",
      "        2751       0.00      0.00      0.00       1.0\n",
      "        2760       0.00      0.00      0.00       1.0\n",
      "        2767       0.00      0.00      0.00       1.0\n",
      "        2779       0.00      0.00      0.00       1.0\n",
      "        2799       0.00      0.00      0.00       0.0\n",
      "        2820       0.00      0.00      0.00       1.0\n",
      "        2825       0.00      0.00      0.00       0.0\n",
      "        2831       0.00      0.00      0.00       1.0\n",
      "        2848       0.00      0.00      0.00       0.0\n",
      "        2859       0.00      0.00      0.00       1.0\n",
      "        2862       0.00      0.00      0.00       1.0\n",
      "        2864       0.00      0.00      0.00       1.0\n",
      "        2896       0.00      0.00      0.00       1.0\n",
      "        2899       0.00      0.00      0.00       0.0\n",
      "        2901       0.00      0.00      0.00       1.0\n",
      "        2978       0.00      0.00      0.00       0.0\n",
      "        3001       0.00      0.00      0.00       0.0\n",
      "        3017       0.00      0.00      0.00       1.0\n",
      "        3021       0.00      0.00      0.00       1.0\n",
      "        3029       0.00      0.00      0.00       1.0\n",
      "        3049       0.00      0.00      0.00       0.0\n",
      "        3069       0.00      0.00      0.00       1.0\n",
      "        3074       0.00      0.00      0.00       1.0\n",
      "        3077       0.00      0.00      0.00       2.0\n",
      "        3079       0.00      0.00      0.00       1.0\n",
      "        3105       0.00      0.00      0.00       1.0\n",
      "        3124       0.00      0.00      0.00       0.0\n",
      "        3161       0.00      0.00      0.00       1.0\n",
      "        3181       0.00      0.00      0.00       0.0\n",
      "        3186       0.00      0.00      0.00       0.0\n",
      "        3190       0.00      0.00      0.00       1.0\n",
      "        3234       0.00      0.00      0.00       1.0\n",
      "        3235       0.00      0.00      0.00       1.0\n",
      "        3244       0.00      0.00      0.00       0.0\n",
      "        3331       0.00      0.00      0.00       1.0\n",
      "        3343       0.00      0.00      0.00       0.0\n",
      "        3368       0.00      0.00      0.00       1.0\n",
      "        3394       0.00      0.00      0.00       0.0\n",
      "        3399       0.00      0.00      0.00       1.0\n",
      "        3414       0.00      0.00      0.00       1.0\n",
      "        3416       0.00      0.00      0.00       0.0\n",
      "        3422       0.00      0.00      0.00       0.0\n",
      "        3447       0.00      0.00      0.00       1.0\n",
      "        3485       0.00      0.00      0.00       1.0\n",
      "        3496       0.00      0.00      0.00       0.0\n",
      "        3512       0.00      0.00      0.00       1.0\n",
      "        3527       0.00      0.00      0.00       0.0\n",
      "        3577       0.00      0.00      0.00       1.0\n",
      "        3578       0.00      0.00      0.00       0.0\n",
      "        3590       0.00      0.00      0.00       0.0\n",
      "        3595       0.00      0.00      0.00       0.0\n",
      "        3599       0.00      0.00      0.00       0.0\n",
      "        3617       0.00      0.00      0.00       0.0\n",
      "        3632       0.00      0.00      0.00       1.0\n",
      "        3643       0.00      0.00      0.00       1.0\n",
      "        3651       0.00      0.00      0.00       1.0\n",
      "        3652       0.00      0.00      0.00       0.0\n",
      "        3656       0.00      0.00      0.00       0.0\n",
      "        3676       0.00      0.00      0.00       1.0\n",
      "        3749       0.00      0.00      0.00       1.0\n",
      "        3757       0.00      0.00      0.00       0.0\n",
      "        3863       0.00      0.00      0.00       1.0\n",
      "        3949       0.00      0.00      0.00       0.0\n",
      "        3959       0.00      0.00      0.00       0.0\n",
      "        3965       0.00      0.00      0.00       1.0\n",
      "        3966       0.00      0.00      0.00       1.0\n",
      "        3972       0.00      0.00      0.00       1.0\n",
      "        3976       0.00      0.00      0.00       1.0\n",
      "        4006       0.00      0.00      0.00       0.0\n",
      "        4057       0.00      0.00      0.00       1.0\n",
      "        4139       0.00      0.00      0.00       1.0\n",
      "        4153       0.00      0.00      0.00       1.0\n",
      "        4210       0.00      0.00      0.00       0.0\n",
      "        4272       0.00      0.00      0.00       1.0\n",
      "        4297       0.00      0.00      0.00       1.0\n",
      "        4380       0.00      0.00      0.00       1.0\n",
      "        4454       0.00      0.00      0.00       0.0\n",
      "        4455       0.00      0.00      0.00       0.0\n",
      "        4473       0.00      0.00      0.00       1.0\n",
      "        4526       0.00      0.00      0.00       0.0\n",
      "        4675       0.00      0.00      0.00       1.0\n",
      "        4788       0.00      0.00      0.00       0.0\n",
      "        4843       0.00      0.00      0.00       1.0\n",
      "        5179       0.00      0.00      0.00       0.0\n",
      "        5381       0.00      0.00      0.00       1.0\n",
      "        5507       0.00      0.00      0.00       0.0\n",
      "        5595       0.00      0.00      0.00       1.0\n",
      "        5771       0.00      0.00      0.00       1.0\n",
      "        5800       0.00      0.00      0.00       1.0\n",
      "        5804       0.00      0.00      0.00       0.0\n",
      "        5866       0.00      0.00      0.00       0.0\n",
      "        5943       0.00      0.00      0.00       0.0\n",
      "        6070       0.00      0.00      0.00       1.0\n",
      "        6078       0.00      0.00      0.00       1.0\n",
      "        6143       0.00      0.00      0.00       1.0\n",
      "        6224       0.00      0.00      0.00       0.0\n",
      "        6229       0.00      0.00      0.00       1.0\n",
      "        6260       0.00      0.00      0.00       0.0\n",
      "        6288       0.00      0.00      0.00       1.0\n",
      "        6289       0.00      0.00      0.00       1.0\n",
      "        6350       0.00      0.00      0.00       1.0\n",
      "        6419       0.00      0.00      0.00       1.0\n",
      "        6527       0.00      0.00      0.00       0.0\n",
      "        6560       0.00      0.00      0.00       0.0\n",
      "        6568       0.00      0.00      0.00       1.0\n",
      "        6681       0.00      0.00      0.00       1.0\n",
      "        6742       0.00      0.00      0.00       1.0\n",
      "        6761       0.00      0.00      0.00       0.0\n",
      "        6836       0.00      0.00      0.00       0.0\n",
      "        6872       0.00      0.00      0.00       1.0\n",
      "        6887       0.00      0.00      0.00       0.0\n",
      "        7119       0.00      0.00      0.00       1.0\n",
      "        7166       0.00      0.00      0.00       0.0\n",
      "        7253       0.00      0.00      0.00       1.0\n",
      "        7297       0.00      0.00      0.00       1.0\n",
      "        7374       0.00      0.00      0.00       0.0\n",
      "        7408       0.00      0.00      0.00       1.0\n",
      "        7476       0.00      0.00      0.00       1.0\n",
      "        7629       0.00      0.00      0.00       1.0\n",
      "        7855       0.00      0.00      0.00       0.0\n",
      "        7865       0.00      0.00      0.00       0.0\n",
      "        7882       0.00      0.00      0.00       0.0\n",
      "        7966       0.00      0.00      0.00       0.0\n",
      "        7980       0.00      0.00      0.00       0.0\n",
      "        8072       0.00      0.00      0.00       0.0\n",
      "        8133       0.00      0.00      0.00       1.0\n",
      "        8386       0.00      0.00      0.00       0.0\n",
      "        8471       0.00      0.00      0.00       0.0\n",
      "        8588       0.00      0.00      0.00       0.0\n",
      "        8613       0.00      0.00      0.00       1.0\n",
      "        8858       0.00      0.00      0.00       1.0\n",
      "        9436       0.00      0.00      0.00       1.0\n",
      "       10297       0.00      0.00      0.00       0.0\n",
      "       10366       0.00      0.00      0.00       0.0\n",
      "       10477       0.00      0.00      0.00       1.0\n",
      "       10722       0.00      0.00      0.00       0.0\n",
      "       11054       0.00      0.00      0.00       0.0\n",
      "       11816       0.00      0.00      0.00       0.0\n",
      "       12389       0.00      0.00      0.00       0.0\n",
      "       14027       0.00      0.00      0.00       1.0\n",
      "       14318       0.00      0.00      0.00       0.0\n",
      "       14421       0.00      0.00      0.00       1.0\n",
      "       14896       0.00      0.00      0.00       1.0\n",
      "       15672       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00     200.0\n",
      "   macro avg       0.00      0.00      0.00     200.0\n",
      "weighted avg       0.00      0.00      0.00     200.0\n",
      "\n",
      "Could not calculate ROC-AUC: Number of classes in y_true not equal to the number of columns in 'y_score'\n",
      "MSE: 4015930.080895964\n",
      "R²: 0.3399192534633566\n",
      "\n",
      "Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         276       0.00      0.00      0.00       1.0\n",
      "         368       0.00      0.00      0.00       1.0\n",
      "         385       0.00      0.00      0.00       1.0\n",
      "         433       0.00      0.00      0.00       1.0\n",
      "         458       0.00      0.00      0.00       1.0\n",
      "         484       0.00      0.00      0.00       1.0\n",
      "         518       0.00      0.00      0.00       1.0\n",
      "         585       0.00      0.00      0.00       1.0\n",
      "         590       0.00      0.00      0.00       0.0\n",
      "         601       0.00      0.00      0.00       1.0\n",
      "         609       0.00      0.00      0.00       1.0\n",
      "         626       0.00      0.00      0.00       1.0\n",
      "         629       0.00      0.00      0.00       0.0\n",
      "         660       0.00      0.00      0.00       1.0\n",
      "         662       0.00      0.00      0.00       1.0\n",
      "         672       0.00      0.00      0.00       0.0\n",
      "         684       0.00      0.00      0.00       1.0\n",
      "         685       0.00      0.00      0.00       0.0\n",
      "         700       0.00      0.00      0.00       0.0\n",
      "         701       0.00      0.00      0.00       1.0\n",
      "         708       0.00      0.00      0.00       0.0\n",
      "         709       0.00      0.00      0.00       1.0\n",
      "         717       0.00      0.00      0.00       0.0\n",
      "         727       0.00      0.00      0.00       1.0\n",
      "         731       0.00      0.00      0.00       1.0\n",
      "         745       0.00      0.00      0.00       1.0\n",
      "         763       0.00      0.00      0.00       0.0\n",
      "         766       0.00      0.00      0.00       1.0\n",
      "         783       0.00      0.00      0.00       1.0\n",
      "         790       0.00      0.00      0.00       0.0\n",
      "         802       0.00      0.00      0.00       1.0\n",
      "         836       0.00      0.00      0.00       0.0\n",
      "         846       0.00      0.00      0.00       1.0\n",
      "         882       0.00      0.00      0.00       1.0\n",
      "         884       0.00      0.00      0.00       1.0\n",
      "         886       0.00      0.00      0.00       0.0\n",
      "         888       0.00      0.00      0.00       1.0\n",
      "         902       0.00      0.00      0.00       0.0\n",
      "         907       0.00      0.00      0.00       1.0\n",
      "         918       0.00      0.00      0.00       1.0\n",
      "         926       0.00      0.00      0.00       1.0\n",
      "         929       0.00      0.00      0.00       1.0\n",
      "         931       0.00      0.00      0.00       1.0\n",
      "         932       0.00      0.00      0.00       1.0\n",
      "         936       0.00      0.00      0.00       0.0\n",
      "         959       0.00      0.00      0.00       1.0\n",
      "         976       0.00      0.00      0.00       0.0\n",
      "        1007       0.00      0.00      0.00       1.0\n",
      "        1024       0.00      0.00      0.00       1.0\n",
      "        1048       0.00      0.00      0.00       1.0\n",
      "        1049       0.00      0.00      0.00       0.0\n",
      "        1098       0.00      0.00      0.00       1.0\n",
      "        1101       0.00      0.00      0.00       1.0\n",
      "        1123       0.00      0.00      0.00       0.0\n",
      "        1126       0.00      0.00      0.00       0.0\n",
      "        1136       0.00      0.00      0.00       0.0\n",
      "        1138       0.00      0.00      0.00       1.0\n",
      "        1154       0.00      0.00      0.00       1.0\n",
      "        1164       0.00      0.00      0.00       1.0\n",
      "        1168       0.00      0.00      0.00       0.0\n",
      "        1169       0.00      0.00      0.00       0.0\n",
      "        1190       0.00      0.00      0.00       1.0\n",
      "        1199       0.00      0.00      0.00       1.0\n",
      "        1200       0.00      0.00      0.00       1.0\n",
      "        1204       0.00      0.00      0.00       1.0\n",
      "        1224       0.00      0.00      0.00       0.0\n",
      "        1236       0.00      0.00      0.00       0.0\n",
      "        1237       0.00      0.00      0.00       0.0\n",
      "        1238       0.00      0.00      0.00       0.0\n",
      "        1244       0.00      0.00      0.00       1.0\n",
      "        1246       0.00      0.00      0.00       1.0\n",
      "        1258       0.00      0.00      0.00       0.0\n",
      "        1262       0.00      0.00      0.00       1.0\n",
      "        1264       0.00      0.00      0.00       0.0\n",
      "        1275       0.00      0.00      0.00       1.0\n",
      "        1278       0.00      0.00      0.00       0.0\n",
      "        1283       0.00      0.00      0.00       1.0\n",
      "        1285       0.00      0.00      0.00       0.0\n",
      "        1287       0.00      0.00      0.00       0.0\n",
      "        1295       0.00      0.00      0.00       2.0\n",
      "        1297       0.00      0.00      0.00       1.0\n",
      "        1318       0.00      0.00      0.00       0.0\n",
      "        1323       0.00      0.00      0.00       1.0\n",
      "        1331       0.00      0.00      0.00       0.0\n",
      "        1337       0.00      0.00      0.00       0.0\n",
      "        1338       0.00      0.00      0.00       1.0\n",
      "        1358       0.00      0.00      0.00       0.0\n",
      "        1361       0.00      0.00      0.00       0.0\n",
      "        1364       0.00      0.00      0.00       0.0\n",
      "        1374       0.00      0.00      0.00       1.0\n",
      "        1376       0.00      0.00      0.00       1.0\n",
      "        1386       0.00      0.00      0.00       0.0\n",
      "        1391       0.00      0.00      0.00       1.0\n",
      "        1393       0.00      0.00      0.00       1.0\n",
      "        1403       0.00      0.00      0.00       0.0\n",
      "        1409       0.00      0.00      0.00       0.0\n",
      "        1410       0.00      0.00      0.00       0.0\n",
      "        1413       0.00      0.00      0.00       1.0\n",
      "        1418       0.00      0.00      0.00       0.0\n",
      "        1433       0.00      0.00      0.00       1.0\n",
      "        1449       0.00      0.00      0.00       1.0\n",
      "        1453       0.00      0.00      0.00       1.0\n",
      "        1455       0.00      0.00      0.00       0.0\n",
      "        1474       0.00      0.00      0.00       0.0\n",
      "        1478       0.00      0.00      0.00       1.0\n",
      "        1494       0.00      0.00      0.00       1.0\n",
      "        1498       0.00      0.00      0.00       0.0\n",
      "        1503       0.00      0.00      0.00       1.0\n",
      "        1520       0.00      0.00      0.00       0.0\n",
      "        1521       0.00      0.00      0.00       0.0\n",
      "        1525       0.00      0.00      0.00       1.0\n",
      "        1530       0.00      0.00      0.00       1.0\n",
      "        1532       0.00      0.00      0.00       1.0\n",
      "        1533       0.00      0.00      0.00       1.0\n",
      "        1534       0.00      0.00      0.00       0.0\n",
      "        1538       0.00      0.00      0.00       0.0\n",
      "        1542       0.00      0.00      0.00       1.0\n",
      "        1544       0.00      0.00      0.00       0.0\n",
      "        1546       0.00      0.00      0.00       1.0\n",
      "        1574       0.00      0.00      0.00       1.0\n",
      "        1597       0.00      0.00      0.00       1.0\n",
      "        1603       0.00      0.00      0.00       1.0\n",
      "        1655       0.00      0.00      0.00       0.0\n",
      "        1657       0.00      0.00      0.00       1.0\n",
      "        1736       0.00      0.00      0.00       1.0\n",
      "        1743       0.00      0.00      0.00       1.0\n",
      "        1766       0.00      0.00      0.00       1.0\n",
      "        1768       0.00      0.00      0.00       0.0\n",
      "        1804       0.00      0.00      0.00       1.0\n",
      "        1817       0.00      0.00      0.00       0.0\n",
      "        1823       0.00      0.00      0.00       1.0\n",
      "        1835       0.00      0.00      0.00       1.0\n",
      "        1845       0.00      0.00      0.00       1.0\n",
      "        1851       0.00      0.00      0.00       1.0\n",
      "        1860       0.00      0.00      0.00       0.0\n",
      "        1864       0.00      0.00      0.00       0.0\n",
      "        1867       0.00      0.00      0.00       0.0\n",
      "        1872       0.00      0.00      0.00       1.0\n",
      "        1880       0.00      0.00      0.00       1.0\n",
      "        1881       0.00      0.00      0.00       1.0\n",
      "        1908       0.00      0.00      0.00       1.0\n",
      "        1913       0.00      0.00      0.00       1.0\n",
      "        1919       0.00      0.00      0.00       0.0\n",
      "        1922       0.00      0.00      0.00       0.0\n",
      "        1925       0.00      0.00      0.00       0.0\n",
      "        1928       0.00      0.00      0.00       1.0\n",
      "        1935       0.00      0.00      0.00       0.0\n",
      "        1938       0.00      0.00      0.00       1.0\n",
      "        1957       0.00      0.00      0.00       0.0\n",
      "        2012       0.00      0.00      0.00       1.0\n",
      "        2030       0.00      0.00      0.00       1.0\n",
      "        2069       0.00      0.00      0.00       0.0\n",
      "        2116       0.00      0.00      0.00       1.0\n",
      "        2122       0.00      0.00      0.00       1.0\n",
      "        2124       0.00      0.00      0.00       1.0\n",
      "        2133       0.00      0.00      0.00       1.0\n",
      "        2141       0.00      0.00      0.00       0.0\n",
      "        2171       0.00      0.00      0.00       2.0\n",
      "        2181       0.00      0.00      0.00       0.0\n",
      "        2186       0.00      0.00      0.00       0.0\n",
      "        2214       0.00      0.00      0.00       0.0\n",
      "        2221       0.00      0.00      0.00       1.0\n",
      "        2225       0.00      0.00      0.00       1.0\n",
      "        2241       0.00      0.00      0.00       1.0\n",
      "        2246       0.00      0.00      0.00       1.0\n",
      "        2249       0.00      0.00      0.00       1.0\n",
      "        2302       0.00      0.00      0.00       1.0\n",
      "        2303       0.00      0.00      0.00       1.0\n",
      "        2323       0.00      0.00      0.00       1.0\n",
      "        2325       0.00      0.00      0.00       1.0\n",
      "        2327       0.00      0.00      0.00       0.0\n",
      "        2384       0.00      0.00      0.00       2.0\n",
      "        2389       0.00      0.00      0.00       1.0\n",
      "        2390       0.00      0.00      0.00       0.0\n",
      "        2406       0.00      0.00      0.00       0.0\n",
      "        2415       0.00      0.00      0.00       0.0\n",
      "        2427       0.00      0.00      0.00       1.0\n",
      "        2463       0.00      0.00      0.00       1.0\n",
      "        2511       0.00      0.00      0.00       0.0\n",
      "        2515       0.00      0.00      0.00       1.0\n",
      "        2538       0.00      0.00      0.00       1.0\n",
      "        2570       0.00      0.00      0.00       0.0\n",
      "        2576       0.00      0.00      0.00       0.0\n",
      "        2600       0.00      0.00      0.00       0.0\n",
      "        2622       0.00      0.00      0.00       0.0\n",
      "        2629       0.00      0.00      0.00       0.0\n",
      "        2684       0.00      0.00      0.00       1.0\n",
      "        2687       0.00      0.00      0.00       0.0\n",
      "        2697       0.00      0.00      0.00       1.0\n",
      "        2708       0.00      0.00      0.00       0.0\n",
      "        2712       0.00      0.00      0.00       1.0\n",
      "        2745       0.00      0.00      0.00       1.0\n",
      "        2751       0.00      0.00      0.00       1.0\n",
      "        2760       0.00      0.00      0.00       1.0\n",
      "        2767       0.00      0.00      0.00       1.0\n",
      "        2779       0.00      0.00      0.00       1.0\n",
      "        2799       0.00      0.00      0.00       0.0\n",
      "        2820       0.00      0.00      0.00       1.0\n",
      "        2825       0.00      0.00      0.00       0.0\n",
      "        2831       0.00      0.00      0.00       1.0\n",
      "        2848       0.00      0.00      0.00       0.0\n",
      "        2859       0.00      0.00      0.00       1.0\n",
      "        2862       0.00      0.00      0.00       1.0\n",
      "        2864       0.00      0.00      0.00       1.0\n",
      "        2896       0.00      0.00      0.00       1.0\n",
      "        2899       0.00      0.00      0.00       0.0\n",
      "        2901       0.00      0.00      0.00       1.0\n",
      "        2978       0.00      0.00      0.00       0.0\n",
      "        3001       0.00      0.00      0.00       0.0\n",
      "        3017       0.00      0.00      0.00       1.0\n",
      "        3021       0.00      0.00      0.00       1.0\n",
      "        3029       0.00      0.00      0.00       1.0\n",
      "        3049       0.00      0.00      0.00       0.0\n",
      "        3069       0.00      0.00      0.00       1.0\n",
      "        3074       0.00      0.00      0.00       1.0\n",
      "        3077       0.00      0.00      0.00       2.0\n",
      "        3079       0.00      0.00      0.00       1.0\n",
      "        3105       0.00      0.00      0.00       1.0\n",
      "        3124       0.00      0.00      0.00       0.0\n",
      "        3161       0.00      0.00      0.00       1.0\n",
      "        3181       0.00      0.00      0.00       0.0\n",
      "        3186       0.00      0.00      0.00       0.0\n",
      "        3190       0.00      0.00      0.00       1.0\n",
      "        3234       0.00      0.00      0.00       1.0\n",
      "        3235       0.00      0.00      0.00       1.0\n",
      "        3244       0.00      0.00      0.00       0.0\n",
      "        3331       0.00      0.00      0.00       1.0\n",
      "        3343       0.00      0.00      0.00       0.0\n",
      "        3368       0.00      0.00      0.00       1.0\n",
      "        3394       0.00      0.00      0.00       0.0\n",
      "        3399       0.00      0.00      0.00       1.0\n",
      "        3414       0.00      0.00      0.00       1.0\n",
      "        3416       0.00      0.00      0.00       0.0\n",
      "        3422       0.00      0.00      0.00       0.0\n",
      "        3447       0.00      0.00      0.00       1.0\n",
      "        3485       0.00      0.00      0.00       1.0\n",
      "        3496       0.00      0.00      0.00       0.0\n",
      "        3512       0.00      0.00      0.00       1.0\n",
      "        3527       0.00      0.00      0.00       0.0\n",
      "        3577       0.00      0.00      0.00       1.0\n",
      "        3578       0.00      0.00      0.00       0.0\n",
      "        3590       0.00      0.00      0.00       0.0\n",
      "        3595       0.00      0.00      0.00       0.0\n",
      "        3599       0.00      0.00      0.00       0.0\n",
      "        3617       0.00      0.00      0.00       0.0\n",
      "        3632       0.00      0.00      0.00       1.0\n",
      "        3643       0.00      0.00      0.00       1.0\n",
      "        3651       0.00      0.00      0.00       1.0\n",
      "        3652       0.00      0.00      0.00       0.0\n",
      "        3656       0.00      0.00      0.00       0.0\n",
      "        3676       0.00      0.00      0.00       1.0\n",
      "        3749       0.00      0.00      0.00       1.0\n",
      "        3757       0.00      0.00      0.00       0.0\n",
      "        3863       0.00      0.00      0.00       1.0\n",
      "        3949       0.00      0.00      0.00       0.0\n",
      "        3959       0.00      0.00      0.00       0.0\n",
      "        3965       0.00      0.00      0.00       1.0\n",
      "        3966       0.00      0.00      0.00       1.0\n",
      "        3972       0.00      0.00      0.00       1.0\n",
      "        3976       0.00      0.00      0.00       1.0\n",
      "        4006       0.00      0.00      0.00       0.0\n",
      "        4057       0.00      0.00      0.00       1.0\n",
      "        4139       0.00      0.00      0.00       1.0\n",
      "        4153       0.00      0.00      0.00       1.0\n",
      "        4210       0.00      0.00      0.00       0.0\n",
      "        4272       0.00      0.00      0.00       1.0\n",
      "        4297       0.00      0.00      0.00       1.0\n",
      "        4380       0.00      0.00      0.00       1.0\n",
      "        4454       0.00      0.00      0.00       0.0\n",
      "        4455       0.00      0.00      0.00       0.0\n",
      "        4473       0.00      0.00      0.00       1.0\n",
      "        4526       0.00      0.00      0.00       0.0\n",
      "        4675       0.00      0.00      0.00       1.0\n",
      "        4788       0.00      0.00      0.00       0.0\n",
      "        4843       0.00      0.00      0.00       1.0\n",
      "        5179       0.00      0.00      0.00       0.0\n",
      "        5381       0.00      0.00      0.00       1.0\n",
      "        5507       0.00      0.00      0.00       0.0\n",
      "        5595       0.00      0.00      0.00       1.0\n",
      "        5771       0.00      0.00      0.00       1.0\n",
      "        5800       0.00      0.00      0.00       1.0\n",
      "        5804       0.00      0.00      0.00       0.0\n",
      "        5866       0.00      0.00      0.00       0.0\n",
      "        5943       0.00      0.00      0.00       0.0\n",
      "        6070       0.00      0.00      0.00       1.0\n",
      "        6078       0.00      0.00      0.00       1.0\n",
      "        6143       0.00      0.00      0.00       1.0\n",
      "        6224       0.00      0.00      0.00       0.0\n",
      "        6229       0.00      0.00      0.00       1.0\n",
      "        6260       0.00      0.00      0.00       0.0\n",
      "        6288       0.00      0.00      0.00       1.0\n",
      "        6289       0.00      0.00      0.00       1.0\n",
      "        6350       0.00      0.00      0.00       1.0\n",
      "        6419       0.00      0.00      0.00       1.0\n",
      "        6527       0.00      0.00      0.00       0.0\n",
      "        6560       0.00      0.00      0.00       0.0\n",
      "        6568       0.00      0.00      0.00       1.0\n",
      "        6681       0.00      0.00      0.00       1.0\n",
      "        6742       0.00      0.00      0.00       1.0\n",
      "        6761       0.00      0.00      0.00       0.0\n",
      "        6836       0.00      0.00      0.00       0.0\n",
      "        6872       0.00      0.00      0.00       1.0\n",
      "        6887       0.00      0.00      0.00       0.0\n",
      "        7119       0.00      0.00      0.00       1.0\n",
      "        7166       0.00      0.00      0.00       0.0\n",
      "        7253       0.00      0.00      0.00       1.0\n",
      "        7297       0.00      0.00      0.00       1.0\n",
      "        7374       0.00      0.00      0.00       0.0\n",
      "        7408       0.00      0.00      0.00       1.0\n",
      "        7476       0.00      0.00      0.00       1.0\n",
      "        7629       0.00      0.00      0.00       1.0\n",
      "        7855       0.00      0.00      0.00       0.0\n",
      "        7865       0.00      0.00      0.00       0.0\n",
      "        7882       0.00      0.00      0.00       0.0\n",
      "        7966       0.00      0.00      0.00       0.0\n",
      "        7980       0.00      0.00      0.00       0.0\n",
      "        8072       0.00      0.00      0.00       0.0\n",
      "        8133       0.00      0.00      0.00       1.0\n",
      "        8386       0.00      0.00      0.00       0.0\n",
      "        8471       0.00      0.00      0.00       0.0\n",
      "        8588       0.00      0.00      0.00       0.0\n",
      "        8613       0.00      0.00      0.00       1.0\n",
      "        8858       0.00      0.00      0.00       1.0\n",
      "        9436       0.00      0.00      0.00       1.0\n",
      "       10297       0.00      0.00      0.00       0.0\n",
      "       10366       0.00      0.00      0.00       0.0\n",
      "       10477       0.00      0.00      0.00       1.0\n",
      "       10722       0.00      0.00      0.00       0.0\n",
      "       11054       0.00      0.00      0.00       0.0\n",
      "       11816       0.00      0.00      0.00       0.0\n",
      "       12389       0.00      0.00      0.00       0.0\n",
      "       14027       0.00      0.00      0.00       1.0\n",
      "       14318       0.00      0.00      0.00       0.0\n",
      "       14421       0.00      0.00      0.00       1.0\n",
      "       14896       0.00      0.00      0.00       1.0\n",
      "       15672       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00     200.0\n",
      "   macro avg       0.00      0.00      0.00     200.0\n",
      "weighted avg       0.00      0.00      0.00     200.0\n",
      "\n",
      "ROC-AUC: <function roc_auc_score at 0x0000018EC3E9C2C0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:663: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  y_type = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# 1️⃣ Check target\n",
    "print(\"Unique labels in y_train:\", len(set(y_train)))\n",
    "print(\"Total samples in y_train:\", len(y_train))\n",
    "\n",
    "# 2️⃣ Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 3️⃣ Train model with more iterations\n",
    "lr_model = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 4️⃣ Predictions\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "y_prob_lr = lr_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# 5️⃣ ROC-AUC handling for binary or multi-class\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "\n",
    "print(\"\\nLogistic Regression\")\n",
    "print(classification_report(y_test, y_pred_lr, zero_division=0))\n",
    "\n",
    "try:\n",
    "    if len(set(y_test)) == 2:  # Binary classification\n",
    "        roc_auc = roc_auc_score(y_test, y_prob_lr[:, 1])\n",
    "    else:  # Multi-class classification\n",
    "        roc_auc = roc_auc_score(y_test, y_prob_lr, multi_class='ovr')\n",
    "    print(\"ROC-AUC:\", roc_auc)\n",
    "except Exception as e:\n",
    "    print(\"Could not calculate ROC-AUC:\", e)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R²:\", r2_score(y_test, y_pred))\n",
    "\n",
    "# 6️⃣ Report\n",
    "print(\"\\nLogistic Regression\")\n",
    "print(classification_report(y_test, y_pred_lr, zero_division=0))\n",
    "print(\"ROC-AUC:\", roc_auc_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeaeb62",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c5203a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         276       0.00      0.00      0.00       1.0\n",
      "         338       0.00      0.00      0.00       0.0\n",
      "         368       0.00      0.00      0.00       1.0\n",
      "         385       0.00      0.00      0.00       1.0\n",
      "         433       0.00      0.00      0.00       1.0\n",
      "         458       0.00      0.00      0.00       1.0\n",
      "         484       0.00      0.00      0.00       1.0\n",
      "         518       0.00      0.00      0.00       1.0\n",
      "         585       0.00      0.00      0.00       1.0\n",
      "         601       0.00      0.00      0.00       1.0\n",
      "         609       0.00      0.00      0.00       1.0\n",
      "         626       0.00      0.00      0.00       1.0\n",
      "         640       0.00      0.00      0.00       0.0\n",
      "         652       0.00      0.00      0.00       0.0\n",
      "         654       0.00      0.00      0.00       0.0\n",
      "         660       0.00      0.00      0.00       1.0\n",
      "         662       0.00      0.00      0.00       1.0\n",
      "         682       0.00      0.00      0.00       0.0\n",
      "         684       0.00      0.00      0.00       1.0\n",
      "         700       0.00      0.00      0.00       0.0\n",
      "         701       0.00      0.00      0.00       1.0\n",
      "         708       0.00      0.00      0.00       0.0\n",
      "         709       0.00      0.00      0.00       1.0\n",
      "         717       0.00      0.00      0.00       0.0\n",
      "         727       0.00      0.00      0.00       1.0\n",
      "         730       0.00      0.00      0.00       0.0\n",
      "         731       0.00      0.00      0.00       1.0\n",
      "         745       0.00      0.00      0.00       1.0\n",
      "         760       0.00      0.00      0.00       0.0\n",
      "         763       0.00      0.00      0.00       0.0\n",
      "         766       0.00      0.00      0.00       1.0\n",
      "         783       0.00      0.00      0.00       1.0\n",
      "         790       0.00      0.00      0.00       0.0\n",
      "         802       0.00      0.00      0.00       1.0\n",
      "         804       0.00      0.00      0.00       0.0\n",
      "         846       0.00      0.00      0.00       1.0\n",
      "         882       0.00      0.00      0.00       1.0\n",
      "         884       0.00      0.00      0.00       1.0\n",
      "         888       0.00      0.00      0.00       1.0\n",
      "         902       0.00      0.00      0.00       0.0\n",
      "         907       0.00      0.00      0.00       1.0\n",
      "         918       0.00      0.00      0.00       1.0\n",
      "         926       0.00      0.00      0.00       1.0\n",
      "         929       0.00      0.00      0.00       1.0\n",
      "         931       0.00      0.00      0.00       1.0\n",
      "         932       0.00      0.00      0.00       1.0\n",
      "         937       0.00      0.00      0.00       0.0\n",
      "         959       0.00      0.00      0.00       1.0\n",
      "         975       0.00      0.00      0.00       0.0\n",
      "         976       0.00      0.00      0.00       0.0\n",
      "         983       0.00      0.00      0.00       0.0\n",
      "         999       0.00      0.00      0.00       0.0\n",
      "        1007       0.00      0.00      0.00       1.0\n",
      "        1024       0.00      0.00      0.00       1.0\n",
      "        1048       0.00      0.00      0.00       1.0\n",
      "        1050       0.00      0.00      0.00       0.0\n",
      "        1068       0.00      0.00      0.00       0.0\n",
      "        1076       0.00      0.00      0.00       0.0\n",
      "        1092       0.00      0.00      0.00       0.0\n",
      "        1098       0.00      0.00      0.00       1.0\n",
      "        1101       0.00      0.00      0.00       1.0\n",
      "        1138       0.00      0.00      0.00       1.0\n",
      "        1154       0.00      0.00      0.00       1.0\n",
      "        1164       0.00      0.00      0.00       1.0\n",
      "        1190       0.00      0.00      0.00       1.0\n",
      "        1199       0.00      0.00      0.00       1.0\n",
      "        1200       0.00      0.00      0.00       1.0\n",
      "        1204       0.00      0.00      0.00       1.0\n",
      "        1217       0.00      0.00      0.00       0.0\n",
      "        1224       0.00      0.00      0.00       0.0\n",
      "        1228       0.00      0.00      0.00       0.0\n",
      "        1236       0.00      0.00      0.00       0.0\n",
      "        1244       0.00      0.00      0.00       1.0\n",
      "        1246       0.00      0.00      0.00       1.0\n",
      "        1262       0.00      0.00      0.00       1.0\n",
      "        1274       0.00      0.00      0.00       0.0\n",
      "        1275       0.00      0.00      0.00       1.0\n",
      "        1283       0.00      0.00      0.00       1.0\n",
      "        1291       0.00      0.00      0.00       0.0\n",
      "        1295       0.00      0.00      0.00       2.0\n",
      "        1297       0.00      0.00      0.00       1.0\n",
      "        1323       0.00      0.00      0.00       1.0\n",
      "        1330       0.00      0.00      0.00       0.0\n",
      "        1331       0.00      0.00      0.00       0.0\n",
      "        1338       0.00      0.00      0.00       1.0\n",
      "        1344       0.00      0.00      0.00       0.0\n",
      "        1347       0.00      0.00      0.00       0.0\n",
      "        1361       0.00      0.00      0.00       0.0\n",
      "        1364       0.00      0.00      0.00       0.0\n",
      "        1374       0.00      0.00      0.00       1.0\n",
      "        1376       0.00      0.00      0.00       1.0\n",
      "        1381       0.00      0.00      0.00       0.0\n",
      "        1386       0.00      0.00      0.00       0.0\n",
      "        1391       0.00      0.00      0.00       1.0\n",
      "        1393       0.00      0.00      0.00       1.0\n",
      "        1403       0.00      0.00      0.00       0.0\n",
      "        1413       0.00      0.00      0.00       1.0\n",
      "        1414       0.00      0.00      0.00       0.0\n",
      "        1424       0.00      0.00      0.00       0.0\n",
      "        1433       0.00      0.00      0.00       1.0\n",
      "        1449       0.00      0.00      0.00       1.0\n",
      "        1453       0.00      0.00      0.00       1.0\n",
      "        1474       0.00      0.00      0.00       0.0\n",
      "        1478       0.00      0.00      0.00       1.0\n",
      "        1494       0.00      0.00      0.00       1.0\n",
      "        1495       0.00      0.00      0.00       0.0\n",
      "        1498       0.00      0.00      0.00       0.0\n",
      "        1503       0.00      0.00      0.00       1.0\n",
      "        1520       0.00      0.00      0.00       0.0\n",
      "        1525       0.00      0.00      0.00       1.0\n",
      "        1530       0.00      0.00      0.00       1.0\n",
      "        1532       0.00      0.00      0.00       1.0\n",
      "        1533       0.00      0.00      0.00       1.0\n",
      "        1542       0.00      0.00      0.00       1.0\n",
      "        1544       0.00      0.00      0.00       0.0\n",
      "        1546       0.00      0.00      0.00       1.0\n",
      "        1553       0.00      0.00      0.00       0.0\n",
      "        1555       0.00      0.00      0.00       0.0\n",
      "        1559       0.00      0.00      0.00       0.0\n",
      "        1567       0.00      0.00      0.00       0.0\n",
      "        1569       0.00      0.00      0.00       0.0\n",
      "        1574       0.00      0.00      0.00       1.0\n",
      "        1597       0.00      0.00      0.00       1.0\n",
      "        1602       0.00      0.00      0.00       0.0\n",
      "        1603       0.00      0.00      0.00       1.0\n",
      "        1657       0.00      0.00      0.00       1.0\n",
      "        1736       0.00      0.00      0.00       1.0\n",
      "        1743       0.00      0.00      0.00       1.0\n",
      "        1766       0.00      0.00      0.00       1.0\n",
      "        1804       0.00      0.00      0.00       1.0\n",
      "        1819       0.00      0.00      0.00       0.0\n",
      "        1823       0.00      0.00      0.00       1.0\n",
      "        1829       0.00      0.00      0.00       0.0\n",
      "        1835       0.00      0.00      0.00       1.0\n",
      "        1845       0.00      0.00      0.00       1.0\n",
      "        1851       0.00      0.00      0.00       1.0\n",
      "        1867       0.00      0.00      0.00       0.0\n",
      "        1872       0.00      0.00      0.00       1.0\n",
      "        1880       0.00      0.00      0.00       1.0\n",
      "        1881       0.00      0.00      0.00       1.0\n",
      "        1882       0.00      0.00      0.00       0.0\n",
      "        1908       0.00      0.00      0.00       1.0\n",
      "        1913       0.00      0.00      0.00       1.0\n",
      "        1925       0.00      0.00      0.00       0.0\n",
      "        1927       0.00      0.00      0.00       0.0\n",
      "        1928       0.00      0.00      0.00       1.0\n",
      "        1935       0.00      0.00      0.00       0.0\n",
      "        1938       0.00      0.00      0.00       1.0\n",
      "        1957       0.00      0.00      0.00       0.0\n",
      "        1980       0.00      0.00      0.00       0.0\n",
      "        1984       0.00      0.00      0.00       0.0\n",
      "        2012       0.00      0.00      0.00       1.0\n",
      "        2030       0.00      0.00      0.00       1.0\n",
      "        2039       0.00      0.00      0.00       0.0\n",
      "        2101       0.00      0.00      0.00       0.0\n",
      "        2116       0.00      0.00      0.00       1.0\n",
      "        2121       0.00      0.00      0.00       0.0\n",
      "        2122       0.00      0.00      0.00       1.0\n",
      "        2124       0.00      0.00      0.00       1.0\n",
      "        2132       0.00      0.00      0.00       0.0\n",
      "        2133       0.00      0.00      0.00       1.0\n",
      "        2136       0.00      0.00      0.00       0.0\n",
      "        2149       0.00      0.00      0.00       0.0\n",
      "        2171       0.00      0.00      0.00       2.0\n",
      "        2181       0.00      0.00      0.00       0.0\n",
      "        2212       0.00      0.00      0.00       0.0\n",
      "        2221       0.00      0.00      0.00       1.0\n",
      "        2225       0.00      0.00      0.00       1.0\n",
      "        2235       0.00      0.00      0.00       0.0\n",
      "        2238       0.00      0.00      0.00       0.0\n",
      "        2241       0.00      0.00      0.00       1.0\n",
      "        2246       0.00      0.00      0.00       1.0\n",
      "        2249       0.00      0.00      0.00       1.0\n",
      "        2255       0.00      0.00      0.00       0.0\n",
      "        2278       0.00      0.00      0.00       0.0\n",
      "        2302       0.00      0.00      0.00       1.0\n",
      "        2303       0.00      0.00      0.00       1.0\n",
      "        2323       0.00      0.00      0.00       1.0\n",
      "        2325       0.00      0.00      0.00       1.0\n",
      "        2329       0.00      0.00      0.00       0.0\n",
      "        2331       0.00      0.00      0.00       0.0\n",
      "        2337       0.00      0.00      0.00       0.0\n",
      "        2346       0.00      0.00      0.00       0.0\n",
      "        2353       0.00      0.00      0.00       0.0\n",
      "        2360       0.00      0.00      0.00       0.0\n",
      "        2384       0.00      0.00      0.00       2.0\n",
      "        2389       0.00      0.00      0.00       1.0\n",
      "        2394       0.00      0.00      0.00       0.0\n",
      "        2406       0.00      0.00      0.00       0.0\n",
      "        2427       0.00      0.00      0.00       1.0\n",
      "        2463       0.00      0.00      0.00       1.0\n",
      "        2473       0.00      0.00      0.00       0.0\n",
      "        2511       0.00      0.00      0.00       0.0\n",
      "        2515       0.00      0.00      0.00       1.0\n",
      "        2520       0.00      0.00      0.00       0.0\n",
      "        2538       0.00      0.00      0.00       1.0\n",
      "        2569       0.00      0.00      0.00       0.0\n",
      "        2570       0.00      0.00      0.00       0.0\n",
      "        2622       0.00      0.00      0.00       0.0\n",
      "        2631       0.00      0.00      0.00       0.0\n",
      "        2659       0.00      0.00      0.00       0.0\n",
      "        2684       0.00      0.00      0.00       1.0\n",
      "        2697       0.00      0.00      0.00       1.0\n",
      "        2712       0.00      0.00      0.00       1.0\n",
      "        2743       0.00      0.00      0.00       0.0\n",
      "        2745       0.00      0.00      0.00       1.0\n",
      "        2748       0.00      0.00      0.00       0.0\n",
      "        2751       0.00      0.00      0.00       1.0\n",
      "        2759       0.00      0.00      0.00       0.0\n",
      "        2760       0.00      0.00      0.00       1.0\n",
      "        2767       0.00      0.00      0.00       1.0\n",
      "        2775       0.00      0.00      0.00       0.0\n",
      "        2779       0.00      0.00      0.00       1.0\n",
      "        2799       0.00      0.00      0.00       0.0\n",
      "        2820       0.00      0.00      0.00       1.0\n",
      "        2825       0.00      0.00      0.00       0.0\n",
      "        2831       0.00      0.00      0.00       1.0\n",
      "        2859       0.00      0.00      0.00       1.0\n",
      "        2862       0.00      0.00      0.00       1.0\n",
      "        2864       0.00      0.00      0.00       1.0\n",
      "        2872       0.00      0.00      0.00       0.0\n",
      "        2892       0.00      0.00      0.00       0.0\n",
      "        2896       0.00      0.00      0.00       1.0\n",
      "        2901       0.00      0.00      0.00       1.0\n",
      "        3017       0.00      0.00      0.00       1.0\n",
      "        3021       0.00      0.00      0.00       1.0\n",
      "        3029       0.00      0.00      0.00       1.0\n",
      "        3069       0.00      0.00      0.00       1.0\n",
      "        3074       0.00      0.00      0.00       1.0\n",
      "        3077       0.00      0.00      0.00       2.0\n",
      "        3079       0.00      0.00      0.00       1.0\n",
      "        3105       0.00      0.00      0.00       1.0\n",
      "        3114       0.00      0.00      0.00       0.0\n",
      "        3149       0.00      0.00      0.00       0.0\n",
      "        3161       0.00      0.00      0.00       1.0\n",
      "        3190       0.00      0.00      0.00       1.0\n",
      "        3234       0.00      0.00      0.00       1.0\n",
      "        3235       0.00      0.00      0.00       1.0\n",
      "        3331       0.00      0.00      0.00       1.0\n",
      "        3368       0.00      0.00      0.00       1.0\n",
      "        3378       0.00      0.00      0.00       0.0\n",
      "        3380       0.00      0.00      0.00       0.0\n",
      "        3384       0.00      0.00      0.00       0.0\n",
      "        3399       0.00      0.00      0.00       1.0\n",
      "        3414       0.00      0.00      0.00       1.0\n",
      "        3447       0.00      0.00      0.00       1.0\n",
      "        3485       0.00      0.00      0.00       1.0\n",
      "        3512       0.00      0.00      0.00       1.0\n",
      "        3518       0.00      0.00      0.00       0.0\n",
      "        3527       0.00      0.00      0.00       0.0\n",
      "        3552       0.00      0.00      0.00       0.0\n",
      "        3565       0.00      0.00      0.00       0.0\n",
      "        3577       0.00      0.00      0.00       1.0\n",
      "        3590       0.00      0.00      0.00       0.0\n",
      "        3595       0.00      0.00      0.00       0.0\n",
      "        3599       0.00      0.00      0.00       0.0\n",
      "        3632       0.00      0.00      0.00       1.0\n",
      "        3643       0.00      0.00      0.00       1.0\n",
      "        3650       0.00      0.00      0.00       0.0\n",
      "        3651       0.00      0.00      0.00       1.0\n",
      "        3676       0.00      0.00      0.00       1.0\n",
      "        3749       0.00      0.00      0.00       1.0\n",
      "        3835       0.00      0.00      0.00       0.0\n",
      "        3863       0.00      0.00      0.00       1.0\n",
      "        3872       0.00      0.00      0.00       0.0\n",
      "        3914       0.00      0.00      0.00       0.0\n",
      "        3965       0.00      0.00      0.00       1.0\n",
      "        3966       0.00      0.00      0.00       1.0\n",
      "        3972       0.00      0.00      0.00       1.0\n",
      "        3976       0.00      0.00      0.00       1.0\n",
      "        4020       0.00      0.00      0.00       0.0\n",
      "        4042       0.00      0.00      0.00       0.0\n",
      "        4057       0.00      0.00      0.00       1.0\n",
      "        4139       0.00      0.00      0.00       1.0\n",
      "        4153       0.00      0.00      0.00       1.0\n",
      "        4165       0.00      0.00      0.00       0.0\n",
      "        4210       0.00      0.00      0.00       0.0\n",
      "        4272       0.00      0.00      0.00       1.0\n",
      "        4297       0.00      0.00      0.00       1.0\n",
      "        4370       0.00      0.00      0.00       0.0\n",
      "        4380       0.00      0.00      0.00       1.0\n",
      "        4454       0.00      0.00      0.00       0.0\n",
      "        4463       0.00      0.00      0.00       0.0\n",
      "        4473       0.00      0.00      0.00       1.0\n",
      "        4526       0.00      0.00      0.00       0.0\n",
      "        4594       0.00      0.00      0.00       0.0\n",
      "        4675       0.00      0.00      0.00       1.0\n",
      "        4686       0.00      0.00      0.00       0.0\n",
      "        4811       0.00      0.00      0.00       0.0\n",
      "        4843       0.00      0.00      0.00       1.0\n",
      "        4844       0.00      0.00      0.00       0.0\n",
      "        5045       0.00      0.00      0.00       0.0\n",
      "        5117       0.00      0.00      0.00       0.0\n",
      "        5152       0.00      0.00      0.00       0.0\n",
      "        5179       0.00      0.00      0.00       0.0\n",
      "        5248       0.00      0.00      0.00       0.0\n",
      "        5381       0.00      0.00      0.00       1.0\n",
      "        5595       0.00      0.00      0.00       1.0\n",
      "        5771       0.00      0.00      0.00       1.0\n",
      "        5800       0.00      0.00      0.00       1.0\n",
      "        5842       0.00      0.00      0.00       0.0\n",
      "        5848       0.00      0.00      0.00       0.0\n",
      "        5866       0.00      0.00      0.00       0.0\n",
      "        5954       0.00      0.00      0.00       0.0\n",
      "        6070       0.00      0.00      0.00       1.0\n",
      "        6078       0.00      0.00      0.00       1.0\n",
      "        6143       0.00      0.00      0.00       1.0\n",
      "        6148       0.00      0.00      0.00       0.0\n",
      "        6224       0.00      0.00      0.00       0.0\n",
      "        6229       0.00      0.00      0.00       1.0\n",
      "        6288       0.00      0.00      0.00       1.0\n",
      "        6289       0.00      0.00      0.00       1.0\n",
      "        6331       0.00      0.00      0.00       0.0\n",
      "        6350       0.00      0.00      0.00       1.0\n",
      "        6419       0.00      0.00      0.00       1.0\n",
      "        6560       0.00      0.00      0.00       0.0\n",
      "        6568       0.00      0.00      0.00       1.0\n",
      "        6681       0.00      0.00      0.00       1.0\n",
      "        6742       0.00      0.00      0.00       1.0\n",
      "        6761       0.00      0.00      0.00       0.0\n",
      "        6872       0.00      0.00      0.00       1.0\n",
      "        7119       0.00      0.00      0.00       1.0\n",
      "        7127       0.00      0.00      0.00       0.0\n",
      "        7253       0.00      0.00      0.00       1.0\n",
      "        7297       0.00      0.00      0.00       1.0\n",
      "        7393       0.00      0.00      0.00       0.0\n",
      "        7408       0.00      0.00      0.00       1.0\n",
      "        7409       0.00      0.00      0.00       0.0\n",
      "        7476       0.00      0.00      0.00       1.0\n",
      "        7629       0.00      0.00      0.00       1.0\n",
      "        7721       0.00      0.00      0.00       0.0\n",
      "        7758       0.00      0.00      0.00       0.0\n",
      "        7966       0.00      0.00      0.00       0.0\n",
      "        7980       0.00      0.00      0.00       0.0\n",
      "        8065       0.00      0.00      0.00       0.0\n",
      "        8072       0.00      0.00      0.00       0.0\n",
      "        8133       0.00      0.00      0.00       1.0\n",
      "        8318       0.00      0.00      0.00       0.0\n",
      "        8386       0.00      0.00      0.00       0.0\n",
      "        8613       0.00      0.00      0.00       1.0\n",
      "        8858       0.00      0.00      0.00       1.0\n",
      "        9277       0.00      0.00      0.00       0.0\n",
      "        9436       0.00      0.00      0.00       1.0\n",
      "        9960       0.00      0.00      0.00       0.0\n",
      "       10127       0.00      0.00      0.00       0.0\n",
      "       10477       0.00      0.00      0.00       1.0\n",
      "       11816       0.00      0.00      0.00       0.0\n",
      "       14027       0.00      0.00      0.00       1.0\n",
      "       14318       0.00      0.00      0.00       0.0\n",
      "       14421       0.00      0.00      0.00       1.0\n",
      "       14782       0.00      0.00      0.00       0.0\n",
      "       14896       0.00      0.00      0.00       1.0\n",
      "       18424       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00     200.0\n",
      "   macro avg       0.00      0.00      0.00     200.0\n",
      "weighted avg       0.00      0.00      0.00     200.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:213: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  y_type = type_of_target(y, input_name=\"y\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "\n",
    "#  Identify target column\n",
    "target_col = 'Credit amount'  # Change to your actual target column\n",
    "\n",
    "#  Handle categorical variables (convert to numeric)\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(target_col, axis=1)\n",
    "y = df[target_col]\n",
    "\n",
    "#  Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "#  Predictions\n",
    "y_pred = dt.predict(X_test)\n",
    "y_prob = dt.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 8. Evaluation\n",
    "print(\"Decision Tree Results\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "df['Credit'] = df['Credit amount'].map({'good': 1, 'bad': 0}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef57291",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "98ba4185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df: (1000, 21)\n",
      "Columns in df: ['Unnamed: 0', 'Age', 'Job', 'Credit amount', 'Duration', 'Sex_male', 'Housing_own', 'Housing_rent', 'Saving accounts_moderate', 'Saving accounts_quite rich', 'Saving accounts_rich', 'Checking account_moderate', 'Checking account_rich', 'Purpose_car', 'Purpose_domestic appliances', 'Purpose_education', 'Purpose_furniture/equipment', 'Purpose_radio/TV', 'Purpose_repairs', 'Purpose_vacation/others', 'Credit']\n",
      "   Unnamed: 0  Age  Job  Credit amount  Duration  Sex_male  Housing_own  \\\n",
      "0           0   67    2           1169         6      True         True   \n",
      "1           1   22    2           5951        48     False         True   \n",
      "2           2   49    1           2096        12      True         True   \n",
      "3           3   45    2           7882        42      True        False   \n",
      "4           4   53    2           4870        24      True        False   \n",
      "\n",
      "   Housing_rent  Saving accounts_moderate  Saving accounts_quite rich  ...  \\\n",
      "0         False                     False                       False  ...   \n",
      "1         False                     False                       False  ...   \n",
      "2         False                     False                       False  ...   \n",
      "3         False                     False                       False  ...   \n",
      "4         False                     False                       False  ...   \n",
      "\n",
      "   Checking account_moderate  Checking account_rich  Purpose_car  \\\n",
      "0                      False                  False        False   \n",
      "1                       True                  False        False   \n",
      "2                      False                  False        False   \n",
      "3                      False                  False        False   \n",
      "4                      False                  False         True   \n",
      "\n",
      "   Purpose_domestic appliances  Purpose_education  \\\n",
      "0                        False              False   \n",
      "1                        False              False   \n",
      "2                        False               True   \n",
      "3                        False              False   \n",
      "4                        False              False   \n",
      "\n",
      "   Purpose_furniture/equipment  Purpose_radio/TV  Purpose_repairs  \\\n",
      "0                        False              True            False   \n",
      "1                        False              True            False   \n",
      "2                        False             False            False   \n",
      "3                         True             False            False   \n",
      "4                        False             False            False   \n",
      "\n",
      "   Purpose_vacation/others  Credit  \n",
      "0                    False     NaN  \n",
      "1                    False     NaN  \n",
      "2                    False     NaN  \n",
      "3                    False     NaN  \n",
      "4                    False     NaN  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1. Check if df is empty\n",
    "print(\"Shape of df:\", df.shape)\n",
    "\n",
    "# 2. Check if required columns exist\n",
    "print(\"Columns in df:\", df.columns.tolist())\n",
    "\n",
    "# 3. Look at first few rows\n",
    "print(df.head())\n",
    "\n",
    "# 4. Ensure no NaN issues before splitting\n",
    "df = df.dropna(subset=['Age', 'Job', 'Duration', 'Credit amount'])  # Drop rows where target/important features are missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a3990208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1000, 19)\n",
      "Shape of y: (1000,)\n"
     ]
    }
   ],
   "source": [
    "X = df[['Age', 'Job', 'Duration', 'Sex_male', 'Housing_own', 'Housing_rent',\n",
    "        'Saving accounts_moderate', 'Saving accounts_quite rich', 'Saving accounts_rich',\n",
    "        'Checking account_moderate', 'Checking account_rich', 'Purpose_car',\n",
    "        'Purpose_domestic appliances', 'Purpose_education', 'Purpose_furniture/equipment',\n",
    "        'Purpose_radio/TV', 'Purpose_repairs', 'Purpose_vacation/others', 'Credit']]\n",
    "\n",
    "y = df['Credit amount']\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "68655216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 4338909.491572499\n",
      "RMSE: 2083.0049187585946\n",
      "MAE: 1416.74455\n",
      "R²: 0.2868325497058627\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Train\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"R²:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "128cbffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.57\n",
      "Precision: 0.49277784362530125\n",
      "Recall: 0.46211451778462087\n",
      "F1 Score: 0.4562275764509143\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Convert 'Credit amount' to categories\n",
    "bins = [0, 1000, 3000, df['Credit amount'].max()]\n",
    "labels = ['Low', 'Medium', 'High']\n",
    "df['Credit_Category'] = pd.cut(df['Credit amount'], bins=bins, labels=labels)\n",
    "\n",
    "# Define target\n",
    "y = df['Credit_Category']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred, average='macro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c548843",
   "metadata": {},
   "source": [
    "### model compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6df3d39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\impute\\_base.py:637: UserWarning: Skipping features without any observed values: ['Credit']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\impute\\_base.py:637: UserWarning: Skipping features without any observed values: ['Credit']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Model  R² Score        MAE       RMSE\n",
      "0  Linear Regression    0.3400  1392.7600  2003.8382\n",
      "1      Decision Tree   -0.4816  1913.0900  3002.3302\n",
      "2      Random Forest    0.2334  1424.8649  2159.6856\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Train, predict, and evaluate\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    results.append([\n",
    "        name,\n",
    "        round(r2_score(y_test, preds), 4),  # R² score\n",
    "        round(mean_absolute_error(y_test, preds), 4),  # MAE\n",
    "        round(np.sqrt(mean_squared_error(y_test, preds)), 4)  # RMSE\n",
    "    ])\n",
    "\n",
    "# Display table\n",
    "df_results = pd.DataFrame(results, columns=[\"Model\", \"R² Score\", \"MAE\", \"RMSE\"])\n",
    "print(df_results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beb7bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      5\u001b[0m     preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      6\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend([\n\u001b[0;32m      7\u001b[0m         name,\n\u001b[1;32m----> 8\u001b[0m         \u001b[38;5;28mround\u001b[39m(\u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m4\u001b[39m),\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28mround\u001b[39m(precision_score(y_test, preds, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m4\u001b[39m),\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28mround\u001b[39m(recall_score(y_test, preds, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m4\u001b[39m),\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28mround\u001b[39m(f1_score(y_test, preds, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     12\u001b[0m     ])\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[0;32m     15\u001b[0m df_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1 Score\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    216\u001b[0m         )\n\u001b[0;32m    217\u001b[0m     ):\n\u001b[1;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    228\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:359\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m    358\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m attach_unique(y_true, y_pred)\n\u001b[1;32m--> 359\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:106\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m    103\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    108\u001b[0m             type_true, type_pred\n\u001b[0;32m    109\u001b[0m         )\n\u001b[0;32m    110\u001b[0m     )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    113\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and continuous targets"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Convert continuous target into categories\n",
    "# Adjust bins as per your data range\n",
    "bins = [0, 5000, 10000, float('inf')]  \n",
    "labels = [0, 1, 2]  # 0=Low, 1=Medium, 2=High\n",
    "y_class = pd.cut(y, bins=bins, labels=labels)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_class, test_size=0.2, random_state=42)\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Train, predict, and evaluate\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    results.append([\n",
    "        name,\n",
    "        round(accuracy_score(y_test, preds), 4),\n",
    "        round(precision_score(y_test, preds, average='weighted'), 4),\n",
    "        round(recall_score(y_test, preds, average='weighted'), 4),\n",
    "        round(f1_score(y_test, preds, average='weighted'), 4)\n",
    "    ])\n",
    "\n",
    "# Display results\n",
    "df_results = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"])\n",
    "print(df_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
